1.	Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?

For this project the goal was simple, we were to examine the data of the enron emails corpus and try and classify a way for the machine to identify a potential person of interest from a set of values and behaviors. Enron was a Fortune 500 company before it collapsed due to corporate fraud. The learning that can be obtained from this dataset can be used for a multitude of things, from spam monitoring to fraud detection.
Breakdown for the dataset:
Total number of data points: 146
Number of Persons of Interest: 18
Number of people without Person of Interest label: 128
Each person has 21 features available

There are some features who are missing lots of value Here is a breakdown of them:
Number of Missing Values for Each Feature:
salary: 51
to_messages: 60
deferral_payments: 107
total_payments: 21
exercised_stock_options: 44
bonus: 64
restricted_stock: 36
shared_receipt_with_poi: 60
restricted_stock_deferred: 128
total_stock_value: 20
expenses: 51
loan_advances: 142
from_messages: 60
other: 53
from_this_person_to_poi: 60
director_fees: 129
deferred_income: 97
long_term_incentive: 80
email_address: 35
from_poi_to_this_person: 60

There were three outliers in this dataset. After identifying the outliers I determined that I had to remove them. So I removed total, the travel agency in the park, and eugene lockhart.


2.	What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.

I ended up creating a function that iterated throught k values starting at 1 and goes through 18 features. I then put the results into an array. I then summed up the precision and recall and the highest combined value was how i selected the K for the final test. This function determined that the best k was a value of 9 so for this project I ended up using ['salary', 'total_payments', 'bonus', 'total_stock_value', 'fraction_to_poi', 'exercised_stock_options', 'deferred_income', 'long_term_incentive', 'combined_poi_communications']. I utilized the SelectKBest feature selection. 
{'salary': 18.289684043404513,
'total_payments': 9.283873618427373,
'bonus': 20.792252047181535,
'total_stock_value': 22.510549090242055,
'fraction_to_poi': 16.40971254803578,
'exercised_stock_options': 22.348975407306217,
'deferred_income': 11.424891485418364,
'long_term_incentive': 9.922186013189823,
'combined_poi_communications': 15.778960003994115}
We can see that the top performing features are. We can see that total_stock_value is one of the biggest indicators, followed by bonus. Two features that I engineer actually showed up in kBest as combined_poi_communication and fraction_to_poi. 

For my engineered fields, I decided that it would be a good idea to create fractions of the total communication that the person conversed with a poi. I started by taking the number of email to poi and divided it by total number of emails. I did this for the from emails as well. Last thing I did was add the two together to get a number that encompassed all POI communication. After feature engineering, two of my features are now in the top 10 list. This tells me that I was on the right track for adding the features on the dataset.

3.	What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?

I tried many different algorithms for this project, I tried KMeans, SVC, RandomForrest, KNeighbors, MLPC, and AdaBoost. I ended up selecting the linear regression algorithm as it gave us the best precision and recall of all the algorithms that I tested. Linear Regression is used in a multitude of fields and I can see why. It is excellent at classifying people based on the data. It had a Precision value of .375 and a recall of .485 both are over the required .3 result. The other algorithms I tried didn’t get to the point where the recall and precision were above .3 so that tells me that they wouldn’t work. It was challenging because sometimes the Precision would be high but the recall low and vice versa. Balancing the two was definitely a challenge.

4.	What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm?

Paramenter tuning is the process of adjusting the values given to the classifier. It changes the behavior of the classifier, allowing you to adjust it to your specific needs. There is a fine line while tuning, you don’t want to tune it too much because the classifier starts to get biased to your testing data.

For this particular algorithm, I ended up passing values for tolerance, C, and random_state. The tolerance I went with was 0.00001 and the C value of 0.0002. the random state was 42 just to keep the results consistent through multiple test. 

5.	What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?

Validation is a set of techniques to ensure that our classifier is accurate to the test data. A common mistake would be overtuning your classifier based on the training data then it doesn’t perform well on the test data. In order to ensure that I didn’t make a classic mistake, I made sure I continually tested the classifier against both sets of data. I wanted to make sure that I did this in order to keep the classifier working for any data in the corpus. To overcome this, we can conduct cross-validation (provided by the evaluate function in poi_id.py where I start 1000 trials and divided the dataset into 3:1 training-to-test ratio. Main reason why we would use StratifiedSuffleSplit rather than other splitting techniques avaible is due to the nature of our dataset, which is extremely small with only 14 Persons of Interest. A single split into a training & test set would not give a better estimate of error accuracy. Therefore, we need to randomly split the data into multiple trials while keeping the fraction of POIs in each trials relatively constant.

6.	Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.

For this project I focused on Precision and Recall. My average rating of precision was 0.375 and for recall was 0.485. Basically speaking, precision refers to the percentage that the classifier correctly identifies know POIs. So if the Classifier predicted 100 POIs the classifier would get it correct 37.5% of the time. Similarly Recall is POI’s correctly identified over the POI’s correctly identified plus POI’s not correctly labeled. Meaning with a recall rate of 48.5% that the classifier gets if correct 48.5% of the time but also misses the POI’s 51.5% of the time. 
